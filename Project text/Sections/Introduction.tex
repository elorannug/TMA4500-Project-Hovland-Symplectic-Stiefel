\section{Introduction}\label{sec:Introduction}

% Intro
In this report we will investigate algorithms that perform optimization on Riemannian manifolds. As noted in the preface of \cite{Boumal2023}, because of the inherent structure of some problems, they can be described as problems on some smooth manifold. This could be because of the problem's natural geometry, latent data simplicity, symmetry and/or positivity. Formulating problems through the lens of smooth manifolds has proven successful in the fields of machine learning, computer vision and robotics, signal processing, scientific computing \cite{Boumal2023}. Optimization on manifolds is a field started around fifty years ago, and has in resent years garnered more attention \cite{Boumal2023}. Optimization on Riemannian manifolds, or simply Riemannian optimization, is a field of optimization where, in an effort to optimize some cost function, one restricts the search space to a Riemannian manifold $\mathcal{M}$. Specifically, instead of defining the problem in terms of restrictions on a Euclidean space, one defines the problem on an appropriate Riemannian manifold. The result of this is that we are, in a sense, encoding the restrictions into inherent properties of the search space. To this end one defines the cost function to be a smooth function inherently defined the manifold, $f\colon\mathcal{M}\to\mathbb{R}$. This results in the following, now unconstrained, optimization problem:
\begin{equation}\label{eq:general_optimization_problem}
    \min_{p\in\mathcal{M}}f(p).
\end{equation}
To solve this problem we employ optimization algorithms adapted to work inherently on the Riemannian manifold. Many of these algorithms can, among other things, exploit the information contained in the smoothness in the cost function. An algorithm is called a first order algorithm if it only utilizes the gradient of the cost function, or of second order if it also leverages the Hessian, as a means to find the critical point of \eqref{eq:general_optimization_problem} more efficiently. Despite this, in applications, the computational cost of computing the gradient and Hessian can be expensive. This means that, for algorithms using the gradient or Hessian in every iteration, the time to compute each iteration can be higher than that of an algorithm not relying on smoothness information. However, as each iteration could be a better guess, in the sense that the iteration moves us closer to the critical point, the total number of iterations are usually lower for higher order algorithms \cite[p.~119]{Boumal2023}. For some problems the number of iterations is so low as to compensate for the higher computational cost per iteration, while for other problems the less accurate but faster lower order algorithms are the optimal choice. 

% Original work
The main goal of this project report is to reproduce some of the findings, programmed in the programming language MATLAB, of Jensen and Zimmermann \cite{JensenZimmermann2024} while programming the experiments in the programming language Julia. To this end we investigate the performance of the first order algorithm "gradient descent" and the second order algorithm called the "trust-region method". We apply these algorithms to problems defined on the symplectic Stiefel manifold, where we solve the problem of finding the nearest symplectic matrix to a given non-symplectic matrix. We also investigate the symplectic decomposition problem. The secondary goal of this report is to investigate the performance of different retractions on algorithm performance, investigated by Bendokat and Zimmermann \cite{BendokatZimmermann2021}. For completeness we provide a partial summary of the theory outlined in both of these papers. Regarding original works, some proofs have been worked out independently by the author of this report. Also, regarding the retraction called pseudo-Riemannian geodesic, investigated in \cite{BendokatZimmermann2021}, we investigate it further than what was done in the original paper. It will be stated explicitly when something is original work. 

% Structure of the report
The structure of the report is as follows. In Section \ref{sec:Introductory_theory}, foundational definitions are described, as well as necessary introductions to the symplectic group and the symplectic Stiefel manifold. The foundational definitions is the only part of the report formatted as a reference work, which means that this section consists of disjoint definitions coining the notation used in the report, as well as serving as a refresher to the underlying theory of the rest of the report. Section \ref{sec:right_invariant_framework} outlines the theory which is the main contribution of \cite{BendokatZimmermann2021}, namely arriving at explicit geodesic equations through defining an appropriate right-invariant metric. We then use these results to define the Riemannian gradient and Hessian before finishing the section with some particular results bridging some gaps from the theory to application. After the theory has been outlined, Section \ref{sec:Algorithms} introduces the Riemannian gradient descent and the Riemannian trust region method. Lastly the section comments on how the algorithms for the experiments were implemented. Section \ref{sec:Numerical_experiments} contains the description and results of the experiments retraction comparison through the nearest symplectic matrix problem, comparing the performance of the Riemannian gradient descent- and Riemannian trust-region algorithm through the nearest symplectic matrix problem, and finally comparing the aforementioned algorithms through the problem of performing a proper symplectic decomposition. Lastly Section \ref{sec:Conclusion} contains the discussion and conclusion of the report, where we summarize our findings, discuss the results, and suggest directions of further work.  

This project report was written in \LaTeX, based on the template made by Jon Arnt Kårstad \cite{Kårstad2024}.