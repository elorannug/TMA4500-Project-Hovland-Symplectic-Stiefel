\begin{abstract}
    This report explores optimization algorithms on Riemannian manifolds, which are problems where the cost function is defined on a smooth manifold. Riemannian optimization is used in many problems, for example in machine learning, robotics and scientific computing. The report reproduces some results from Jensen and Zimmermann \cite{JensenZimmermann2024} and Bendokat and Zimmermann \cite{BendokatZimmermann2021}. In particular we investigate the Riemannian gradient descent method and the Riemannian trust-region method. To compare the algorithms we apply them to multiple optimization problems of symplectic nature, as well as evaluating the impact of different retractions on the algorithms performance. All of the code have been implemented in Julia using the package Manopt.jl \cite{Bergmann2022}. Apart from mostly reaffirming the results in the aforementioned papers, we also find that the pseudo-Riemannian geodesic retraction, introduced by Bendokat and Zimmermann, performs competitively compared to the Cayley retraction.
\end{abstract}